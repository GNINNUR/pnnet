* PN-Net:  Conjoined Triple Deep Network for Learning Local Image Descriptors

[[http://arxiv.org/abs/1601.05030][Arxiv Page]]



** Intro
Code for the arxiv paper [[http://arxiv.org/pdf/1601.05030v1][PN-Net:  Conjoined Triple Deep Network for Learning Local Image Descriptors]].

The network extracts feature descriptors from grayscale local patches
of size =32x32=.

** Architecture
#+begin_src bash
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> output]
  (1): cudnn.SpatialConvolution(1 -> 32, 7x7)
  (2): cudnn.Tanh
  (3): cudnn.SpatialMaxPooling(2,2,2,2)
  (4): cudnn.SpatialConvolution(32 -> 64, 6x6)
  (5): cudnn.Tanh
  (6): nn.View
  (7): nn.Linear(4096 -> 128)
  (8): cudnn.Tanh
}
#+end_src

** Implementation details
For optimization details refer to the arxiv publication. Training code
is now also available.

** How to use 

Download the sample dataset from
http://vbalnt.io/notredame-torch.tar.gz and extract

Run =th eval.lua=

** How to train 

Check that the file =notredame.t7= is available.  
To get it run =wget http://vbalnt.io/notredame-torch.tar.gz= and
extract.

In the =train= folder, run =th run.lua=. When training with =1.2M=
triplets on a =GTX TITAN X=, each epoch takes approximately /2mins/

Examples of the training triplets used
[[./triplets.png]]

** Positive matching examples 
Examples of positive nearest neighbour patch matching using the pnnet 
descriptor in the  [[http://www.robots.ox.ac.uk/~vgg/research/affine/][Oxford matching dataset]].

[[./true_positives.png]]


** Efficiency 
Efficiency comparison with [[https://github.com/hanxf/matchnet][MatchNet]] and [[https://github.com/szagoruyko/cvpr15deepcompare][deepcompare]], both from
CVPR 2015. For more results refer to the arxiv paper.

[[./efficiency.png]]

